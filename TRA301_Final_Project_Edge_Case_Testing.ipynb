{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization Step\n",
        "Installing transformers, configuring torch and tokenizer.\n",
        "⚠️ Ensure that the GPU is enabled for the Colab runtime running this notebook."
      ],
      "metadata": {
        "id": "LsXJaPNxFLBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers==4.25.1\n",
        "import torch\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi711jw9FPK8",
        "outputId": "12e00627-b8c0-4b76-bee3-8dfe7d762b79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.25.1 in /usr/local/lib/python3.10/dist-packages (4.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (0.14.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (3.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (23.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (2023.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.1) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.1) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Generation: Prompt 1 and 2\n",
        "\n",
        "Prompt: Create a function that sorts a list of integers in descending order\n",
        "\n",
        "Note that code generated on subsequent runs might not be identical to stored output in this file.\n"
      ],
      "metadata": {
        "id": "tzvR5HWQFSOC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6qIcUOdtrAP",
        "outputId": "830a4cd4-7775-4487-a0a5-dfcbe479ce54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/codegen/modeling_codegen.py:167: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)\n",
            "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output for prompt: This function sorts a list of integers in descending order\n",
            "# This function sorts a list of integers in descending order\n",
            "# Input: a list of integers\n",
            "# Output: a list of integers in descending order\n",
            "def descending_order(list):\n",
            "    for i in range(len(list)):\n",
            "        for j in range(len(list)-1):\n",
            "            if list[j] > list[j+1]:\n",
            "                list[j], list[j+1] = list[j+1], list[j]\n",
            "    return list\n",
            "\n",
            "Output for prompt: This function sorts the given array by using the fastest sorting algorithm for small arrays.\n",
            "# This function sorts the given array by using the fastest sorting algorithm for small arrays.\n",
            "# It uses the merge sort algorithm for large arrays.\n",
            "#\n",
            "# Time Complexity: O(n log n)\n",
            "# Space Complexity: O(n)\n",
            "def mergeSort(arr):\n",
            "    if len(arr) > 1:\n",
            "        mid = len(arr) // 2\n",
            "        L = arr[:mid]\n",
            "        R = arr[mid:]\n",
            "\n",
            "        mergeSort(L)\n",
            "        mergeSort(R)\n",
            "\n",
            "        i = j = k = 0\n",
            "\n",
            "        while i < len(L) and j < len(R):\n",
            "            if L[i] < R[j]:\n",
            "                arr[k] = L[i]\n",
            "                i += 1\n",
            "            else:\n",
            "                arr[k] = R[j]\n",
            "                j += 1\n",
            "            k += 1\n",
            "\n",
            "        while i < len(L):\n",
            "            arr[k] = L[i]\n",
            "            i += 1\n",
            "            k += 1\n",
            "\n",
            "        while j < len(R):\n",
            "            arr[k] = R[j]\n",
            "            j += 1\n",
            "            k += 1\n"
          ]
        }
      ],
      "source": [
        "# Configuring tokenizer and model to be run\n",
        "# This configuration uses the lower-medium sized CodeGen model trained on a\n",
        "# corpus of Python code data\n",
        "# More model config options at https://github.com/salesforce/CodeGen\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-mono\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-2B-mono\")\n",
        "\n",
        "# Specifying prompts for code generation to be run on\n",
        "prompts = [\"This function sorts a list of integers in descending order\",\n",
        "           \"This function sorts the given array by using the fastest sorting algorithm for small arrays.\"]\n",
        "\n",
        "for prompt in prompts:\n",
        "  current_input = (\"# \" + prompt)\n",
        "  inputs = tokenizer(current_input, return_tensors=\"pt\").to(0)\n",
        "  sample = model.generate(**inputs, max_length=2048)\n",
        "  print((\"\\nOutput for prompt: \" + prompt))\n",
        "  print(tokenizer.decode(sample[0], truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f4b2YRW1M7ak"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}
